import requests
import whisper
from loguru import logger
import time
import torch


from src.constants import INTERVIEW_POSITION, OUTPUT_FILE_NAME, LLAMA_SERVER_URL

SYSTEM_PROMPT = (
    f"–¢—ã –ø—Ä–æ—Ö–æ–¥–∏—à—å —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞ –¥–æ–ª–∂–Ω–æ—Å—Ç—å {INTERVIEW_POSITION} –≤ –†–æ—Å—Å–∏–∏.\n"
    "–¢–µ–±–µ –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–ø–æ–ª–Ω—ã–º –∏–ª–∏ –Ω–µ–º–Ω–æ–≥–æ –∏—Å–∫–∞–∂—ë–Ω–Ω—ã–º ‚Äî –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è –ø–æ–Ω—è—Ç—å —Å—É—Ç—å –∏ –¥–∞—Ç—å —Ä–∞–∑–≤—ë—Ä–Ω—É—Ç—ã–π –æ—Ç–≤–µ—Ç.\n"
    "–û—Ç–≤–µ—á–∞–π –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞, –∫–∞–∫ –±—É–¥—Ç–æ —Ç—ã –∫–∞–Ω–¥–∏–¥–∞—Ç. –ö–∞–∂–¥—ã–π –æ—Ç–≤–µ—Ç —Å–æ–ø—Ä–æ–≤–æ–∂–¥–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏–∑ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–ª–∏ –≥–∏–ø–æ—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏–π.\n"
    "–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º ‚Äî –Ω–µ –±–æ–ª–µ–µ 150 —Å–ª–æ–≤."
)

device = "cuda" if torch.cuda.is_available() else "cpu"
_WHISPER_MODEL = whisper.load_model("medium", device=device) 

project_id = "bf69751b-65af-4457-9a4c-a8d9453a6b06"
token = "87ce6187b84d0168781527c126b1769e"

def transcribe_audio(path_to_file: str = OUTPUT_FILE_NAME) -> str:
    """
    –õ–æ–∫–∞–ª—å–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ openai-whisper.

    Args:
        path_to_file (str): –ü—É—Ç—å –¥–æ .wav-—Ñ–∞–π–ª–∞ –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.

    Returns:
        str: –†–∞—Å—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç.
    """
    start = time.time()  # ‚è±Ô∏è –Ω–∞—á–∏–Ω–∞–µ–º –∑–∞–º–µ—Ä –≤—Ä–µ–º–µ–Ω–∏
    # –≤—ã–Ω—É–∂–¥–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–∞–µ–º fp16 –Ω–∞ CPU / WSL, –µ—Å–ª–∏ –Ω–µ—Ç GPU
    result = _WHISPER_MODEL.transcribe(path_to_file, fp16=False, language='ru')
    print(f"üìù Transcription took {time.time() - start:.3f} seconds")  # –ª–æ–≥ –≤—Ä–µ–º–µ–Ω–∏
    return result["text"]

def generate_answer(transcript: str, temperature: float = 0.7) -> str:

    headers = {
        'Authorization': f'Bearer {token}',
        'x-project-id': project_id,
            }
   
    

    payload = {
        "messages": [
                     {"role": "system",   "content": SYSTEM_PROMPT},
                     {"role": "user", "content": transcript}
            ],
        "model": "model-run-jbq8l-famous",
        "frequency_penalty": 0.1,
        "stream": False,
        "temperature": 0.5,
        "top_p": 0.95,
        "top_k": 10,
        "repetition_penalty": 1.03,
        "length_penalty": 1,
        "max_tokens": 500
        }   

    url = f"{LLAMA_SERVER_URL.rstrip('/')}/v1/chat/completions"
    logger.debug(f"Calling llama-server at {url}")

    res = requests.post(url, headers=headers, json=payload, timeout=60)
    res.raise_for_status()
    data = res.json()

    # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ –ø–µ—Ä–≤–æ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞
    return data["choices"][0]["message"]["content"]
